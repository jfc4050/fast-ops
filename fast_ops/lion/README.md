# Evolved Sign Momentum (Lion) ðŸ˜¾

**See [Symbolic Discovery of Optimization Algorithms](https://arxiv.org/pdf/2302.06675.pdf)**

Fused Lion implementation, written in CUDA. Only tested on an A100 GPU for now,
it'll probably work on other GPUs, just make sure you run the unit tests before
using if you aren't on Ampere.

The paper claims some improved convergence properties, also only has to
track momentum, which would lead to some pretty large memory savings.

If you're coming from AdamW, the paper provides some suggestions for
adjustments to LR and weight decay.
> The update generated by Lion is an element-wise binary Â±1,
as a result of the sign operation, therefore it has a larger norm than
those generated by other optimizers. Based on our experience, a suitable
learning rate for Lion is typically 3-10x smaller than that for AdamW.
Since the effective weight decay is lr * Î»: update += w * Î»; update *= lr,
the value of Î» used for Lion is 3-10x larger than that for AdamW in order
to maintain a similar strength

## Benchmarks
First working version is about 3x faster than the
[vanilla implementation taken from Google AutoML](https://github.com/google/automl/blob/master/lion/lion_pytorch.py).
Gap would probably be smaller if we let `torch.jit` fuse some of the reference
implementation (needs some reworking/rearranging to enable this).
I'll get around to profiling/optimizing later.
```
[------------------ lion step -------------------]
                               |   ours  |   ref
1 threads: ---------------------------------------
      8.2e+03, torch.float16   |   25.1  |    84.5
      8.2e+03, torch.bfloat16  |   25.0  |    84.8
      6.7e+07, torch.float16   |  651.9  |  1878.6
      6.7e+07, torch.bfloat16  |  710.1  |  1889.7

Times are in microseconds (us).
```
